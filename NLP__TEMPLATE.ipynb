{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP _TEMPLATE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSbtgwQ3KEHxzG+xiG52fZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oxidiovega/ML_DL_templates/blob/main/NLP__TEMPLATE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDWSjv6RhpNl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import warnings\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2fQMQdhvCS"
      },
      "source": [
        "df=pd.read_csv(\"\") #insert the path to your csv file \n",
        "df = df.dropna() # dropping NaN values\n",
        "df.head() # checking the first 5 values "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GBgyAgNieYJ"
      },
      "source": [
        "#now it's time to make a wordcloud to vizualize the text\n",
        "text_column=\" \" #the name of the column that contains your text data\n",
        "label_column=\" \" #the name of your labels column\n",
        "stopwords = set(STOPWORDS) #we convert the list of stopwords to a set because looking up an element in a set is faster \n",
        "text = \" \".join(review for review in df[''])#insert the clomun name where your text is located\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXwABZoQjg-X"
      },
      "source": [
        "\n",
        "# we generate the wordcloud\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iour2ex9kKEN"
      },
      "source": [
        "stopwords.update([\" \",\" \"]) #this is to add the other common stopwords that you deduce from the Visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d69pNG6ikTjW"
      },
      "source": [
        "df[text_column] = df[text_column].str.lower()  #we make sure that all the words are in low-case\n",
        "\n",
        "df[text_column]=df[text_column].apply(lambda row:word_tokenize(row)) #we tokenize our sentences into words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m39GaHtxk8aU"
      },
      "source": [
        "df[text_column]=df[text_column].apply(lambda row:[w for w in row if  w.isalpha()]) #to clean the text from the html tags,and non-alphanumerical data\n",
        "df[text_column]=df[text_column].apply(lambda row:[w for w in row if  not w in stopwords]) #we remove the stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6BFvurQlQjk"
      },
      "source": [
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNkZXKRGlYvt"
      },
      "source": [
        "df[text_column]=df[text_column].apply(lambda row : [lemmatizer.lemmatize(word) for word in row ]) #lemmetization version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlYPjOa2lpWG"
      },
      "source": [
        "df[text_column] = df[text_column].apply(lambda row: [snow_stemmer.stem(word) for word in row]) # stemming version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WLGK7AAmQwG"
      },
      "source": [
        "df[text_column]=ds[text_column].apply(lambda x:\" \".join(x)) # we join the strings back into one single string so that we're able to apply TF-IDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GJ8R97cmpB-"
      },
      "source": [
        "df.to_csv(\"dataset_clean.csv\", encoding='utf-8', index=False) # in order to save the work already done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FQme8MKmuyR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvZA7clim52e"
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(df[text_column],df[label_column],test_size=0.3) # we prepare the training /test data to train our model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCkc8GijnPV_"
      },
      "source": [
        "# we use gridsearch to get the best parameters for our model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25,0.35,0.5, 0.75), # you can add more parameters here\n",
        "    'tfidf__min_df': [5,10,15,20,25,50],\n",
        "    \n",
        "    \n",
        "}\n",
        "\n",
        "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
        "grid_search_tune.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters set:\")\n",
        "print (grid_search_tune.best_estimator_.steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJrSHyNfnUXU"
      },
      "source": [
        "tf=TfidfVectorizer(min_df=,max_df=) # we pick the best parameters based on the grid search result and create an instant of tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gnem52undkB"
      },
      "source": [
        "# we make a pipeline object for our model\n",
        "pipeline_tf=Pipeline([('tf-idf',tf),\n",
        "           ('clf',LinearSVC()) #this is the model that we're going to use , you can replace it with Knn or any other classifier\n",
        "          \n",
        "          ])\n",
        "model_tf=pipeline_tf.fit(X_train,y_train)\n",
        "predicted_tf=model_tf.predict(X_test)\n",
        "report_tf=classification_report(y_test,predicted_tf) #we finally get our results \n",
        "print(report_tf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9D93xC5pnQ1"
      },
      "source": [
        "now we're going to experiment with cosine distance and looking for similaire sentences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGnp6dP1pzDt"
      },
      "source": [
        "# we're going to use this function to preprocess our sentence\n",
        "def preprocess_phrase(phrase):\n",
        "  \n",
        "  \n",
        "  phrase=phrase.lower() # we lower the case\n",
        "  phrase=phrase.split() # this is to tokenize the sentence\n",
        "  ls=[]\n",
        "\n",
        "  for i in phrase: # and to remove non alphabetical words and stopwords\n",
        "    if(i.isalpha() and i not in stopwords):\n",
        "      ls.append(lemmatizer.lemmatize(i)) # we lemmatize the word \n",
        "\n",
        "  ls=\" \".join(ls)\n",
        "  return ls\n",
        "\n",
        "preprocess_phrase(\" try typing stuff here /*/+.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_hzqDqgqeOq"
      },
      "source": [
        "def get_similaire_tf(phrase):\n",
        "  preprocess_phrase(phrase)\n",
        "  new_row = {'text_column':phrase} # we make a row out of our phrase\n",
        "  new_df=df.append(new_row,ignore_index=True) # we append our own phrase to the dataframe and make a copy of the entire dataframe\n",
        "  X = tf.fit_transform(new_df[text_column]) #we apply tf-idf\n",
        "  cosine_similarities = linear_kernel(X[-1:],X).flatten() # linear_kernel will give us the cosine distance between the sentence we added and every row of our dataframe \n",
        "  #and flatten will turn it into a 1D array\n",
        "  related_docs_indices = cosine_similarities.argsort()[:-6:-1] # this is to sort the 5 most similaire results from our distance array\n",
        "  related_docs_indices\n",
        "  phrases=[]\n",
        "  for i in related_docs_indices:\n",
        "    phrases.append(df[text_column][i])\n",
        "  return phrases\n",
        "  \n",
        "# the idea is to append the phrase ( our input ) to a copy of the dataset and reclaculate the tf-idf and then to use the similarity between the last element( our input) and\n",
        "#the whole dataset and finally sort through the top 5 indices\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-4l4MQdrW1e"
      },
      "source": [
        "get_similaire_tf(\" insert your new sentence here\") # try putting some text in this"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}